{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Leads in Linkedin Scraping Translated: 714\n"
     ]
    }
   ],
   "source": [
    "path = \"data\"\n",
    "filename = \"NewData\"\n",
    "\n",
    "import os, random, sys, time\n",
    "# from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from Detect_New_Entries import detect_new_entires, merge_new_entires\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Scrapes the new data from linkedin \n",
    "# and saves it to the Excel Scraping Translated.xlsx\n",
    "\n",
    "### READ DATA\n",
    "##############################################################\n",
    "leads_df = pd.read_excel('Linkedin_Scraping_Translated.xlsx') \n",
    "original_leads = pd.read_excel(f'{path}/{filename}.xlsx')\n",
    "\n",
    "### LEADS MISSING - NOT CASE SENSITIVE\n",
    "##############################################################\n",
    "original_leads = original_leads.rename({'Company / Account':'Company'}, axis=1)\n",
    "\n",
    "scraped_comp = pd.DataFrame(leads_df['Name'].str.lower().unique(),columns=[\"Comp\"]).dropna()\n",
    "orig_leads = pd.DataFrame(original_leads['Company'].str.lower().unique(),columns=[\"Comp\"]).dropna()\n",
    "\n",
    "to_scrape = orig_leads.merge(scraped_comp,indicator = True, how='left').loc[lambda x : x['_merge']!='both']\n",
    "new_leads = to_scrape['Comp']\n",
    "print(\"New Leads in Linkedin Scraping Translated:\", len(new_leads))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # path = \"data\"\n",
    "# # filename = \"NewData\"\n",
    "\n",
    "# # import os, random, sys, time\n",
    "# # # from urllib.parse import urlparse\n",
    "# # from selenium import webdriver\n",
    "# # from bs4 import BeautifulSoup\n",
    "# # from selenium.webdriver.common.keys import Keys\n",
    "# # from selenium.webdriver.common.by import By\n",
    "# # import pandas as pd\n",
    "# # from selenium.common.exceptions import NoSuchElementException\n",
    "# # from Detect_New_Entries import detect_new_entires, merge_new_entires\n",
    "# # import time\n",
    "# # from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# # Scrapes the new data from linkedin \n",
    "# # and saves it to the Excel Scraping Translated.xlsx\n",
    "\n",
    "# ### READ DATA\n",
    "# ##############################################################\n",
    "# leads_df2 = pd.read_excel('Lead_Scores_Machine_Learning.xlsx') \n",
    "# grid_header2 = ['ResponseId','Pred Prob', 'Explain_1','Explain_2','Explain_3','Explain_4','Explain_5']\n",
    "# leads_df2[grid_header2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_header = ['Name', 'Web_Name', 'Website','Phone','Industry','Company size','Headquarters','Type','Founded','Specialties']\n",
    "# leads_df[grid_header].sample(n = 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ### LOGIN\n",
    "# ##############################################################\n",
    "# browser = webdriver.Chrome('driver/chromedriver83.exe')\n",
    "# browser.get('https://www.linkedin.com/login')\n",
    "\n",
    "# import random\n",
    "# session = random.randint(1,7)\n",
    "\n",
    "# file = open(f'config/config_linkedin_{session}.txt')\n",
    "# lines = file.readlines()\n",
    "# username = lines[0]\n",
    "# password = lines[1]\n",
    "\n",
    "# ##############################################################\n",
    "# elementID = browser.find_element_by_id('username')\n",
    "# elementID.send_keys(username)\n",
    "\n",
    "# elementID = browser.find_element_by_id('password')\n",
    "# elementID.send_keys(password)\n",
    "# elementID.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login_account():\n",
    "    ### LOGIN\n",
    "    ##############################################################\n",
    "    browser = webdriver.Chrome('driver/chromedriver83.exe')\n",
    "    browser.get('https://www.linkedin.com/login')\n",
    "\n",
    "    import random\n",
    "    session = round(random.uniform(1, 6))\n",
    "\n",
    "    file = open(f'config/config_linkedin_{session}.txt')\n",
    "    lines = file.readlines()\n",
    "    username = lines[0]\n",
    "    password = lines[1]\n",
    "\n",
    "    ##############################################################\n",
    "    elementID = browser.find_element_by_id('username')\n",
    "    elementID.send_keys(username)\n",
    "\n",
    "    elementID = browser.find_element_by_id('password')\n",
    "    elementID.send_keys(password)\n",
    "    elementID.submit()\n",
    "    return browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New companies: 714\n"
     ]
    }
   ],
   "source": [
    "# READ\n",
    "##############################################################\n",
    "# leads_df = pd.read_excel('leads_missing_25_Mar_2020.xlsx')\n",
    "\n",
    "company_list = new_leads\n",
    "company_list = company_list.dropna() \n",
    "company_list = company_list.drop_duplicates()\n",
    "company_list = company_list.tolist()\n",
    "company_list = [w.replace(' & ',' and ') for w in company_list]\n",
    "\n",
    "##############################################################\n",
    "companies = company_list[:160]\n",
    "##############################################################\n",
    "print(\"New companies:\", len(company_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# companies_list = []\n",
    "# companies_count = 0\n",
    "# browser = login_account()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity-result__title-line flex-shrink-1\n",
    "# search-result__result-link ember-view\n",
    "# entity-result__title-text t-bold t-16\n",
    "# //a[@id='ember64']//h3[@class='search-result__title t-16 t-black t-bold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for company in companies:\n",
    "\n",
    "#     browser.get('https://www.linkedin.com/')\n",
    "#     comp_search = \"https://www.linkedin.com/search/results/companies/?keywords=\"\n",
    "#     company = company.replace(\"about/\", \"\")\n",
    "#     fullLink_search = comp_search + company\n",
    "#     browser.get(fullLink_search)\n",
    "#     time.sleep(round(random.uniform(6, 9),2))\n",
    "\n",
    "#     error = False\n",
    "\n",
    "#     headers_list = []\n",
    "#     comp_data_list = []\n",
    "\n",
    "#     comp_data_list.append(company)\n",
    "#     headers_list.append('Name')\n",
    "\n",
    "#     # If it does not find the company catch.\n",
    "#     try:\n",
    "# #         browser.find_element_by_xpath(\"//h3[@class='app-aware-link ember-view']\").click()\n",
    "#         venue = browser.find_element_by_xpath('//a[@class=\"app-aware-link ember-view\"]')\n",
    "#         venue.click()\n",
    "#         print(\"Yes\")\n",
    "#     except NoSuchElementException:  #spelling error making this code not work as expected\n",
    "#         print(\"Caca\")\n",
    "#     time.sleep(round(random.uniform(6, 9),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: Scraping: 0.62% completed - boschung international ag\n",
      "Saving the data\n",
      "                        Name      Web_Name                   Website Phone  \\\n",
      "0  boschung international ag          None                      None  None   \n",
      "1               boucledor sa  Boucledor SA  http://www.boucledor.com  None   \n",
      "\n",
      "                 Industry      Company size    Headquarters            Type  \\\n",
      "0                    None              None            None            None   \n",
      "1  Luxury Goods & Jewelry  51-200 employees  Meyrin, Genève  Privately Held   \n",
      "\n",
      "  Founded                                        Specialties  \n",
      "0    None                                               None  \n",
      "1    1965  Horlogerie, Luxe, Fermoirs, Boucles déployante...  \n",
      "Linkedin Scraping Translated and its backups saved correctly.\n",
      "2: Scraping: 1.25% completed - boucledor sa\n",
      "3: Scraping: 1.88% completed - bouygues e&s enertrans ag\n",
      "Saving the data\n",
      "                        Name               Web_Name  \\\n",
      "0  bouygues e&s enertrans ag  Bouygues E&S InTec AG   \n",
      "1          bovet fleurier sa                   None   \n",
      "\n",
      "                       Website Phone      Industry       Company size  \\\n",
      "0  http://bouygues-es-intec.ch  None  Construction  201-500 employees   \n",
      "1                         None  None          None               None   \n",
      "\n",
      "  Headquarters  Type Founded Specialties  \n",
      "0         None  None    None        None  \n",
      "1         None  None    None        None  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-70736af05746>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[0mcompanies_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompanies_count\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcompanies_count\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0msave_the_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[0mcompanies_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;31m#         browser.close()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-71-9c988f7c961e>\u001b[0m in \u001b[0;36msave_the_date\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[0mmix_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmix_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Unnamed: 0'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[0mmix_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmix_file\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mout_grid_header\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[0mmix_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Linkedin_Scraping_Translated.xlsx\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m     \u001b[0mmix_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'backups_scrape/Linkedin_Scraping_Translated{date_string}.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes)\u001b[0m\n\u001b[0;32m   2254\u001b[0m             \u001b[0mstartcol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstartcol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2255\u001b[0m             \u001b[0mfreeze_panes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfreeze_panes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2256\u001b[1;33m             \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2257\u001b[0m         )\n\u001b[0;32m   2258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\excel.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine)\u001b[0m\n\u001b[0;32m    737\u001b[0m             \u001b[0mstartrow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstartrow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m             \u001b[0mstartcol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstartcol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 739\u001b[1;33m             \u001b[0mfreeze_panes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfreeze_panes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    740\u001b[0m         )\n\u001b[0;32m    741\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mneed_save\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_xlsxwriter.py\u001b[0m in \u001b[0;36mwrite_cells\u001b[1;34m(self, cells, sheet_name, startrow, startcol, freeze_panes)\u001b[0m\n\u001b[0;32m    235\u001b[0m                 )\n\u001b[0;32m    236\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 237\u001b[1;33m                 \u001b[0mwks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstartrow\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstartcol\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xlsxwriter\\worksheet.py\u001b[0m in \u001b[0;36mcell_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_args\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcell_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xlsxwriter\\worksheet.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, row, col, *args)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m         \"\"\"\n\u001b[1;32m--> 413\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[1;31m# Undecorated version of write().\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xlsxwriter\\worksheet.py\u001b[0m in \u001b[0;36m_write\u001b[1;34m(self, row, col, *args)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtoken_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnum_types\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_write_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtoken_type\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\xlsxwriter\\worksheet.py\u001b[0m in \u001b[0;36m_write_number\u001b[1;34m(self, row, col, number, cell_format)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[1;31m# Check that row and col are valid and store max and min values.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_dimensions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### LOOP\n",
    "##############################################################\n",
    "\n",
    "companies_list = []\n",
    "companies_count = 0\n",
    "browser = login_account()\n",
    "batch = 2\n",
    "for company in companies:\n",
    "\n",
    "    browser.get('https://www.linkedin.com/')\n",
    "    comp_search = \"https://www.linkedin.com/search/results/companies/?keywords=\"\n",
    "    company = company.replace(\"about/\", \"\")\n",
    "    fullLink_search = comp_search + company\n",
    "    browser.get(fullLink_search)\n",
    "    time.sleep(round(random.uniform(6, 9),2))\n",
    "\n",
    "    error = False\n",
    "\n",
    "    headers_list = []\n",
    "    comp_data_list = []\n",
    "\n",
    "    comp_data_list.append(company)\n",
    "    headers_list.append('Name')\n",
    "\n",
    "    # If it does not find the company catch.\n",
    "    try:\n",
    "#         browser.find_element_by_xpath(\"//h3[@class='app-aware-link ember-view']\").click()\n",
    "        venue = browser.find_element_by_xpath('//a[@class=\"app-aware-link ember-view\"]')\n",
    "        venue.click()\n",
    "        time.sleep(round(random.uniform(1, 2),2))\n",
    "\n",
    "    except NoSuchElementException:  #spelling error making this code not work as expected\n",
    "        companies_list.append(headers_list)\n",
    "        companies_list.append(comp_data_list)\n",
    "        error = True\n",
    "\n",
    "    time.sleep(round(random.uniform(5, 9),2))\n",
    "\n",
    "    if error == False:\n",
    "        current_page = browser.current_url\n",
    "\n",
    "        if \"about/\" in current_page:\n",
    "            pass\n",
    "        else:\n",
    "            fullLink = current_page + \"about/\"\n",
    "            browser.get(fullLink)\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(round(random.uniform(5, 7),2))\n",
    "\n",
    "        src = browser.page_source\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "\n",
    "        try:\n",
    "            comp_data_list.append(browser.find_element_by_xpath(\"//h1[@class='org-top-card-summary__title t-24 t-black truncate']\").text)\n",
    "            headers_list.append('Web_Name')\n",
    "        except NoSuchElementException:  \n",
    "            pass \n",
    "\n",
    "\n",
    "        # Chech if there is anything in the about page\n",
    "        try:\n",
    "            company_div = soup.find('div', {'class': 'org-grid__core-rail--no-margin-left'})\n",
    "            comp = company_div.find_all('p')\n",
    "            comp_desc = comp[0]\n",
    "        except IndexError:  #spelling error making this code not work as expected\n",
    "            companies_list.append(headers_list)\n",
    "            companies_list.append(comp_data_list)\n",
    "            error = True    \n",
    "\n",
    "        if error == False:    \n",
    "            comp_desc = comp[0].get_text().strip() \n",
    "            header = company_div.find_all('dt')\n",
    "            comp_data = company_div.find_all('dd')\n",
    "\n",
    "            j = 0         \n",
    "            for i in range(len(header)):\n",
    "                headers_list.append(header[i].get_text().strip())\n",
    "                comp_data_list.append(comp_data[j].get_text().strip())\n",
    "                if header[i].get_text().strip()==\"Company size\" and len(header) < len(comp_data):\n",
    "                    j += 1  \n",
    "                j += 1\n",
    "\n",
    "            companies_list.append(headers_list)\n",
    "            companies_list.append(comp_data_list)\n",
    "\n",
    "    time.sleep(round(random.uniform(2, 5),2))    \n",
    "    companies_count = companies_count+1\n",
    "    print('{}: Scraping: {}% completed - {}'.format(companies_count,round(companies_count/len(companies)*100,2),company))\n",
    "    \n",
    "    if companies_count % batch == 0:\n",
    "        save_the_date()\n",
    "        companies_list = []\n",
    "        browser = login_account()\n",
    "#         time.sleep(round(random.uniform(250, 280),2))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_the_date():\n",
    "    print(\"Saving the data\")\n",
    "    # ## Transform data from scrapping to DataFrame\n",
    "    ##############################################################\n",
    "    grid_header = ['Name', 'Web_Name', 'Website','Phone','Industry','Company size','Headquarters','Type','Founded','Specialties']\n",
    "\n",
    "    dataframe = []\n",
    "    for k in range(0,batch):\n",
    "        j = 0\n",
    "        i = 0\n",
    "        lista = []\n",
    "\n",
    "        while i< len(grid_header):\n",
    "            try:\n",
    "                if grid_header[i] == companies_list[0+k*2][j]:\n",
    "                    field = companies_list[1+k*2][j]\n",
    "                else:\n",
    "                    field = None\n",
    "                    j -= 1\n",
    "                lista.append(field)\n",
    "                j +=1\n",
    "\n",
    "            except:\n",
    "                lista.append(None)\n",
    "            i += 1    \n",
    "        dataframe.append(lista)\n",
    "\n",
    "    df = pd.DataFrame.from_records(dataframe,columns=grid_header)\n",
    "#     df.iloc[3] = df.iloc[3].str.split('\\n').str[0]\n",
    "    df[\"Name\"] = df[\"Name\"].replace(\" and \",\" & \", regex=True)\n",
    "    \n",
    "    ### SAVE\n",
    "    ##############################################################\n",
    "    prev_file = pd.read_excel(\"Linkedin_Scraping_Translated.xlsx\")\n",
    "    date_string = time.strftime(\"%Y-%m-%d-%Hh\")\n",
    "\n",
    "    out_grid_header = ['Name', 'Web_Name', 'Website','Phone','Industry','Company size',\n",
    "                       'Headquarters','Type','Founded','Specialties','Translated?','Translation']\n",
    "\n",
    "    mix_file = prev_file.append(df)\n",
    "    mix_file = mix_file.drop(columns=['Unnamed: 0'])\n",
    "    mix_file = mix_file[out_grid_header]\n",
    "    mix_file.to_excel(\"Linkedin_Scraping_Translated.xlsx\")\n",
    "    mix_file.to_excel(f'backups_scrape/Linkedin_Scraping_Translated{date_string}.xlsx')\n",
    "\n",
    "    print(\"Linkedin Scraping Translated and its backups saved correctly.\")\n",
    "\n",
    "    browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
